1.设计模式：https://www.bilibili.com/video/BV17z4y117qn?p=6&spm_id_from=pageDriver
2.spring cloud:https://ke.qq.com/webcourse/index.html#cid=3133099&term_id=103256589&taid=10431882859630251&type=1024&vid=5285890809151991097

3.分布式事务：https://ke.qq.com/webcourse/index.html#cid=398381&term_id=100558952&taid=4185905191851053&type=1024&vid=5285890805571759230
	0.XA是Oracle定义的一套协议，驱动会实现XA Connection，RM-resource manager 资源拥有者,指具体的数据库资源;TM-transaction manager,事务管理者
	1.2pc提交：先预备提交，大家都先执行SQL，但是不提交，要提交就一起提交，但是有可能在第二阶段发生某个端挂了，这种小概率时间发生后，需要逆操作模拟回滚。这个事务资源是锁定的，connection不能释放
		* 大家都执行SQL，锁定资源
		* 大家再一起提交（这里会引入一个问题，如果某一个事务commit失败了呢？不可能100%解决，只能提高成功概率，可以通过重试，如果还不成功就记录日志补偿）
		* 本地消息表-发送消息-消费者幂等与应答，做最终一致性
	2.3pc是为了减少资源锁定时间
		* 增加询问过程，先不执行sql，不要锁定资源（预备阶段）
		* pre-commit 此时才锁定资源（准备阶段）
		* 最后都commit（提交阶段）

		* 降低锁定资源的概率
		* tm和rm的超时处理机制，tm未收到反馈rm的中断反馈；rm:在第二阶段和第三阶段的TM中断命令，超时默认提交。
		* 2pc和3pc区别：引入超时机制。同事在协调者（超时、中断事务）和参与者（超时，在pre中断，在do提交）中都引入超时机制。2pc前面加一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的
		引入超时机制：在协调者和参与者中引入超时机制
		细分阶段：把两阶段提交协议的第一个阶段再次细分成询问阶段、和预备阶段

	3.2pc实现框架：bytetcc,tcc-transaction，jdbc自带的驱动也可以实现XA分布式事务，但是这个场景是连接多个数据库，实际微服务架构的场景连接的是多个服务
	4.常用方法：seata、lcn、可靠消息队列
	5.CAP:C-一致性、A-可用性、P-分区容忍性。一般会做出如下选择：保证P和A，舍弃C强一致性，保证最终一致性，分布式系统有很多网络调用，必须要保证P
		* 强一致性：系统在执行过某项操作后仍然处于一致的状态。在分布式系统中，更新操作执行成功后所有的用户都应该读到最新的值，这样的系统被认为是具有强一致性的
	　　* 可用性：每一个操作总是能够在一定的时间内返回结果，这里需要注意的是"一定时间内"和"返回结果"。一定时间指的是，在可以容忍的范围内返回结果，结果可以是成功或者失败。 对数据更新具备高可用性（A）
　　		* 分区容错性：理解为在存在网络分区的情况下，仍然可以接受请求（满足一致性和可用性)。这里的网络分区是指由于某种原因，网络被分成若干个孤立的区域，而区域之间互不相通。还有一些人将分区容错性理解为系统对节点动态加入和离开的能力，因为节点的加入和离开可以认为是集群内部的网络分区。
	6.CAP不保证C的情况下，又引申出BASE理论：Basically Availbale（基本可用），Soft state（软状态）和Eventually（最终一致性），它是CAP的一个扩展，牺牲强一致性来获得可用性，当出现故障允许部分不可用，但要保证核心功能可用，允许数据在一段时间内是不一致的，但最终达到一致，满足BASE理论的事务，我们称之为“柔性事务”
	7.消息队列+本地事件（流水）表（不适用于数据量大），AB系统都有一个事件表，通过消息id（事件id）来保证唯一
		* A系统业务完成，插入事件流水表，以下两步是同一个事务
		> 定时任务读取事件表，查询到“WAIT”状态的数据
		> 把状态update成“SENDED”（未提交事务），定时任务放到消息队列成功，事务提交
		* B系统轮询队列，无需开启事务，插入事件表，返回ack，如果入库失败，mq.recovery()，消费者的一致性是通过队列重复消费，幂等控制的
		> 通过事件id（不一定是消息队列id），主键约束，来保证幂等

		注意点：
		> 配置消息持久化
		> 配置死信队列，保证前面的数据库成功了，但是消息消费有问题，只能通过死信队列补偿处理

	8.LCN:Lock（锁定事务单元）/Confirm（确认事务）/Notify（通知事务），基于2阶段提交，依赖于数据库的事务，代理数据源连接
		> https://github.com/codingapi/tx-lcn
		* 向TxManager发送请求：创建事务组-groupId
		* 真正发起事务调用链
		* 直到事务调用到最后（第N个）事务，服务N执行完成，向TxManager添加到事务组，返回
		* 基于上一步，n-1调用完成，服务n-1添加到事务组，直到返回到发起者
		* 发起者通知事务组TxManager关闭事务并提交
		* 最后TxManager通知其他服务提交或回滚
		* 需要解决问题
			> 协调机制（第一阶段）：第一阶段的connection和第二阶段的connection要一致，需要更多的数据库连接，当业务提交事务的时候，实际并未提交
			> 补偿机制：做标识，记录执行sql，存储到redis，服务自己也要实现补偿接口

	9.tcc:try/confirm/cancel，更类似于是基于多服务调用，而不是基于多数据库调用，适用于无事务状态的服务
		* try 多个事务SQL一并执行
		* 如果try都ok，走confirm，不ok走cancel
		* cancel的逆SQL就是一个坑，针对于insert和delete可以
		* 具体流程
		> 各个事务组件跟TM事务协调者建立长连接
		> 事务发起者starting，向TM创建事务组
		> 调用joiner服务，执行内部逻辑，未提交事务，添加到事务组
		> starting提交或者回滚会通知TM
		> TM会通知其他服务提交或者回滚
		> 整个过程是基于接口交互:try/confirm/cancel

	10.seata
		* 事务协调者：TC-transaction coordinator
		* 模式：AT、TCC、SAGA、XA
		* 本地锁 + 全局锁（同一个业务不同事务的全局锁）
		* 如何解决ABA问题
		* AT模式的执行过程
		> 分布式事务1先获取本地锁并执行SQL，获取全局锁，并提交本地，释放本地锁，接着再调用另一个服务RM，最终全局提交或者回滚，释放全局锁
		* 全局锁实现写隔离，类似"for update"实现读隔离

	11.可靠消息最终一致性方案
		* 四个角色：业务发起者、可靠消息服务+数据库（自己开发），消息状态分为：待确认/待发送/已发送/已完成、MQ
		* A服务执行业务逻辑之前，发送【待确认】消息给【可靠消息服务】，【待确认】消息入库
		* A服务执行业务逻辑，如果成功则发送【确认】消息，失败发送【取消】消息
		* 【待确认】消息更新状态为【已确认】or【已取消】
		* 如果是【已确认】，发送消息到MQ，消息的状态从【已确认】改成【已发送】
		> 响应时间更快，把整条调用链路变成了：调用单体的单个功能
		> 实际相当于把本地消息表的数据存储到可靠消息服务中
		> 如果采用本地消息表的机制，每个服务都要写一套，但是用可靠消息服务就共用一套即可，用类型来区分

	12.尽最大努力消息通知事务
		* 应用场景：我方调用第三方，应用再第三方系统调用中、第三方开发，属于第三方开放平台。被接收方“尽最大努力”通知发起方，如果最终都失败了，发起者只能自己查询

	13.结合事务消息（RocketMQ的消息属性）
